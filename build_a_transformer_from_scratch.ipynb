{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "There have been plenty of well-organized tutorials elaborating on details of the Transformer. This one is inpired by and based on annotated-transformer from the Harvard NLP group, which is a great tutorial showing everything you need to reproduce the transformer model from paper. However, from a beginner's standpoint, it is sometimes easy to get lost when stuck with an unfamiliar concept and need to go for further readings. In this notebook, I try to alleviate this by organizing the codes in a top-down manner. And instead of using texts from the original paper of transfomer, I will explain using my own words and provide links to useful resources for each module if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from feedforward import FeedForwardNetwork\n",
    "from multiheadattention import Multihead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple task\n",
    "Firstly, we want to know what our task is. We take the same task as in annotated-transformer, which is to memorize the sequence of numbers from 1 to 10. Therefore, the size of our vocabulary should be 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_encoder=6, \n",
    "            num_decoder=6, \n",
    "            d_model=512, \n",
    "            vocab_size=10,\n",
    "            num_head=6,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        c = deepcopy()\n",
    "        ffn = FeedForwardNetwork(d_model)\n",
    "        attn = MultiHeadAttention(num_head, d_model)\n",
    "        self.model = EncoderDecoder(\n",
    "            Encoder(EncoderLayer(c(attn), c(ffn)), num_layer=num_encoder),\n",
    "            Decoder(DecoderLayer(c(attn), c(attn), c(ffn)), num_layer=num_decoder),\n",
    "            nn.Sequential(nn.Embedding)\n",
    "        )\n",
    "\n",
    "    def forward(self, ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a1 = nn.Parameter(torch.ones(d_model))\n",
    "        self.b1 = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        # std = torch.std(x, dim=-1, keepdim=True)\n",
    "        std, mean = torch.std_mean(x, dim=-1, keepdim=True)\n",
    "        return self.a1 * (x - mean) / (std + self.eps) + self.b1\n",
    "\n",
    "\n",
    "class SubLayer(nn.Module):\n",
    "    '''\n",
    "        Add & Norm\n",
    "    '''\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.layernorm = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return x + self.layernorm(self.dropout(x))\n",
    "        pass\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(SubLayer):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__(d_model, dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ffn(x)\n",
    "        return x + self.layernorm(self.dropout(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForwardNetwork(512, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNetwork(\n",
       "  (layernorm): LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (ffn): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
