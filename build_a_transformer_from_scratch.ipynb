{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "There have been plenty of well-organized tutorials elaborating on details of the Transformer. This one is inpired by and based on annotated-transformer from the Harvard NLP group, which is a great tutorial showing everything you need to reproduce the transformer model from paper. However, from a beginner's standpoint, it is sometimes easy to get lost when stuck with an unfamiliar concept and need to go for further readings. In this notebook, I try to alleviate this by organizing the codes in a top-down manner. And instead of using texts from the original paper of transfomer, I will explain using my own words and provide links to useful resources for each module if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from feedforward import FeedForwardNetwork\n",
    "from multiheadattention import MultiHeadAttention\n",
    "from utils import clone, PositionalEncoding, Embedding, get_subsequent_mask, rate, greedy_decode, Generator\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple task\n",
    "Firstly, we want to know what our task is. We take the same task as in annotated-transformer, which is to memorize the sequence of numbers from 1 to 10. Therefore, the size of our vocabulary should be 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_encoder=6, \n",
    "            num_decoder=6, \n",
    "            d_model=512, \n",
    "            vocab_size=13,\n",
    "            num_head=6,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        c = deepcopy\n",
    "        ffn = FeedForwardNetwork(d_model)\n",
    "        attn = MultiHeadAttention(d_model=d_model, num_head=num_head)\n",
    "        self.d_model = d_model\n",
    "        self.shared = Embedding(vocab=vocab_size, d_model=d_model)\n",
    "        self.model = EncoderDecoder(\n",
    "            Encoder(EncoderLayer(c(attn), c(ffn)), num_layers=num_encoder),\n",
    "            Decoder(DecoderLayer(c(attn), c(attn), c(ffn)), num_layers=num_decoder),\n",
    "            nn.Sequential(c(self.shared),\n",
    "                          PositionalEncoding(d_model=d_model)),\n",
    "            nn.Sequential(c(self.shared),\n",
    "                          PositionalEncoding(d_model=d_model)),\n",
    "            Generator(d_model=d_model, vocab_size=vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, src_input, tgt_input, src_mask, tgt_mask):\n",
    "        logits = self.model(src_input, tgt_input, src_mask, tgt_mask)\n",
    "        sequence = F.linear(logits, self.shared.embedder.weight)\n",
    "        return (logits, sequence)\n",
    "\n",
    "    def generate(self, x):\n",
    "        return self.model.generator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder, tgt_embedder, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.embedder = embedder\n",
    "        self.tgt_embedder = tgt_embedder\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src_input, tgt_input, src_mask, tgt_mask):\n",
    "        memory = self.encode(src_input, src_mask)\n",
    "        return self.decode(memory, src_mask, tgt_input, tgt_mask)\n",
    "\n",
    "    def encode(self, src_input, src_mask):\n",
    "        return self.encoder(self.embedder(src_input), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt_input, tgt_mask):\n",
    "        return self.decoder(memory, src_mask, self.tgt_embedder(tgt_input), tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layer_list = clone(layer, num_layers)\n",
    "\n",
    "    def forward(self, src_embed, src_mask):\n",
    "        x = src_embed\n",
    "        for layer in self.layer_list:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attn, ffn):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.ffn = ffn\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.attn(x, x, x, mask)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layer_list = clone(layer, num_layers)\n",
    "\n",
    "    def forward(self, memory, src_mask, tgt_embed, tgt_mask):\n",
    "        x = tgt_embed\n",
    "        for layer in self.layer_list:\n",
    "            x = layer(memory, src_mask, tgt_embed, tgt_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, attn, cross_attn, ffn):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.cross_attn = cross_attn\n",
    "        self.ffn = ffn\n",
    "\n",
    "    def forward(self, m, src_mask, x, tgt_mask):\n",
    "        x = self.attn(x, x, x, tgt_mask)\n",
    "        x = self.cross_attn(x, m, m, src_mask, cross=True)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2702,  0.9665,  0.8689,  ...,  0.7461, -1.0133, -1.5127],\n",
      "        [ 1.6205, -0.3668,  0.3662,  ...,  1.4504,  0.2733,  0.2208],\n",
      "        [-0.1541,  0.5831,  0.2178,  ..., -0.5265, -2.2544,  0.5922],\n",
      "        ...,\n",
      "        [ 1.0133, -0.1850, -0.9212,  ..., -0.0677, -0.0754,  0.2580],\n",
      "        [-1.3319, -0.5734, -0.7726,  ...,  1.0976,  1.7588, -0.5740],\n",
      "        [ 0.5696,  0.2808, -0.0281,  ...,  0.7650, -0.3095, -0.6587]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0121,  0.0141, -0.0337,  ...,  0.0385,  0.0301, -0.0140],\n",
      "        [ 0.0023, -0.0131,  0.0261,  ...,  0.0220,  0.0099, -0.0161],\n",
      "        [ 0.0038, -0.0330, -0.0116,  ...,  0.0045,  0.0382, -0.0126],\n",
      "        ...,\n",
      "        [-0.0313,  0.0436, -0.0388,  ..., -0.0292,  0.0174,  0.0217],\n",
      "        [ 0.0297,  0.0240,  0.0338,  ...,  0.0068, -0.0381, -0.0416],\n",
      "        [-0.0107, -0.0324,  0.0367,  ..., -0.0309, -0.0391,  0.0053]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0184, -0.0086, -0.0157,  ...,  0.0023,  0.0253, -0.0204],\n",
      "        [-0.0068,  0.0441,  0.0032,  ...,  0.0231,  0.0020,  0.0126],\n",
      "        [ 0.0090,  0.0101,  0.0398,  ..., -0.0237, -0.0219,  0.0117],\n",
      "        ...,\n",
      "        [ 0.0393, -0.0186, -0.0240,  ...,  0.0116,  0.0074, -0.0261],\n",
      "        [ 0.0023, -0.0234, -0.0347,  ...,  0.0052,  0.0158,  0.0007],\n",
      "        [ 0.0219, -0.0387, -0.0323,  ..., -0.0374,  0.0027, -0.0003]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0091,  0.0035,  0.0308,  ...,  0.0122,  0.0217,  0.0441],\n",
      "        [-0.0336,  0.0283,  0.0159,  ..., -0.0276, -0.0279, -0.0283],\n",
      "        [-0.0290, -0.0334,  0.0398,  ...,  0.0081,  0.0111, -0.0336],\n",
      "        ...,\n",
      "        [ 0.0090,  0.0148, -0.0181,  ...,  0.0432, -0.0372, -0.0116],\n",
      "        [-0.0075, -0.0372,  0.0384,  ..., -0.0432,  0.0198,  0.0269],\n",
      "        [-0.0091,  0.0245, -0.0440,  ..., -0.0320,  0.0202, -0.0384]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0432,  0.0222,  ...,  0.0241,  0.0324,  0.0065],\n",
      "        [-0.0243,  0.0436,  0.0241,  ...,  0.0068,  0.0233, -0.0327],\n",
      "        [ 0.0366, -0.0072,  0.0288,  ...,  0.0299,  0.0147,  0.0269],\n",
      "        ...,\n",
      "        [ 0.0150,  0.0258, -0.0323,  ...,  0.0004, -0.0241, -0.0119],\n",
      "        [-0.0252, -0.0273,  0.0304,  ...,  0.0370, -0.0247,  0.0341],\n",
      "        [ 0.0143, -0.0218,  0.0024,  ..., -0.0072, -0.0113,  0.0059]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0283,  0.0309,  ...,  0.0180,  0.0361,  0.0383],\n",
      "        [-0.0106, -0.0211, -0.0108,  ...,  0.0228, -0.0127, -0.0421],\n",
      "        [ 0.0188,  0.0225,  0.0230,  ..., -0.0391, -0.0187,  0.0121],\n",
      "        ...,\n",
      "        [-0.0186, -0.0052, -0.0300,  ..., -0.0426,  0.0213,  0.0061],\n",
      "        [ 0.0260, -0.0129, -0.0328,  ...,  0.0437, -0.0151, -0.0103],\n",
      "        [-0.0040,  0.0391,  0.0074,  ..., -0.0284,  0.0130, -0.0263]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0197, -0.0189,  0.0169,  ..., -0.0165,  0.0085,  0.0189],\n",
      "        [-0.0020, -0.0051,  0.0152,  ...,  0.0057, -0.0191, -0.0185],\n",
      "        [ 0.0180, -0.0019, -0.0120,  ..., -0.0041, -0.0158, -0.0215],\n",
      "        ...,\n",
      "        [ 0.0166,  0.0182,  0.0216,  ..., -0.0104, -0.0179, -0.0081],\n",
      "        [-0.0135,  0.0183,  0.0034,  ...,  0.0141,  0.0194,  0.0089],\n",
      "        [-0.0108,  0.0064,  0.0002,  ...,  0.0100,  0.0198,  0.0086]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0121,  0.0141, -0.0337,  ...,  0.0385,  0.0301, -0.0140],\n",
      "        [ 0.0023, -0.0131,  0.0261,  ...,  0.0220,  0.0099, -0.0161],\n",
      "        [ 0.0038, -0.0330, -0.0116,  ...,  0.0045,  0.0382, -0.0126],\n",
      "        ...,\n",
      "        [-0.0313,  0.0436, -0.0388,  ..., -0.0292,  0.0174,  0.0217],\n",
      "        [ 0.0297,  0.0240,  0.0338,  ...,  0.0068, -0.0381, -0.0416],\n",
      "        [-0.0107, -0.0324,  0.0367,  ..., -0.0309, -0.0391,  0.0053]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0184, -0.0086, -0.0157,  ...,  0.0023,  0.0253, -0.0204],\n",
      "        [-0.0068,  0.0441,  0.0032,  ...,  0.0231,  0.0020,  0.0126],\n",
      "        [ 0.0090,  0.0101,  0.0398,  ..., -0.0237, -0.0219,  0.0117],\n",
      "        ...,\n",
      "        [ 0.0393, -0.0186, -0.0240,  ...,  0.0116,  0.0074, -0.0261],\n",
      "        [ 0.0023, -0.0234, -0.0347,  ...,  0.0052,  0.0158,  0.0007],\n",
      "        [ 0.0219, -0.0387, -0.0323,  ..., -0.0374,  0.0027, -0.0003]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0091,  0.0035,  0.0308,  ...,  0.0122,  0.0217,  0.0441],\n",
      "        [-0.0336,  0.0283,  0.0159,  ..., -0.0276, -0.0279, -0.0283],\n",
      "        [-0.0290, -0.0334,  0.0398,  ...,  0.0081,  0.0111, -0.0336],\n",
      "        ...,\n",
      "        [ 0.0090,  0.0148, -0.0181,  ...,  0.0432, -0.0372, -0.0116],\n",
      "        [-0.0075, -0.0372,  0.0384,  ..., -0.0432,  0.0198,  0.0269],\n",
      "        [-0.0091,  0.0245, -0.0440,  ..., -0.0320,  0.0202, -0.0384]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0432,  0.0222,  ...,  0.0241,  0.0324,  0.0065],\n",
      "        [-0.0243,  0.0436,  0.0241,  ...,  0.0068,  0.0233, -0.0327],\n",
      "        [ 0.0366, -0.0072,  0.0288,  ...,  0.0299,  0.0147,  0.0269],\n",
      "        ...,\n",
      "        [ 0.0150,  0.0258, -0.0323,  ...,  0.0004, -0.0241, -0.0119],\n",
      "        [-0.0252, -0.0273,  0.0304,  ...,  0.0370, -0.0247,  0.0341],\n",
      "        [ 0.0143, -0.0218,  0.0024,  ..., -0.0072, -0.0113,  0.0059]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0283,  0.0309,  ...,  0.0180,  0.0361,  0.0383],\n",
      "        [-0.0106, -0.0211, -0.0108,  ...,  0.0228, -0.0127, -0.0421],\n",
      "        [ 0.0188,  0.0225,  0.0230,  ..., -0.0391, -0.0187,  0.0121],\n",
      "        ...,\n",
      "        [-0.0186, -0.0052, -0.0300,  ..., -0.0426,  0.0213,  0.0061],\n",
      "        [ 0.0260, -0.0129, -0.0328,  ...,  0.0437, -0.0151, -0.0103],\n",
      "        [-0.0040,  0.0391,  0.0074,  ..., -0.0284,  0.0130, -0.0263]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0197, -0.0189,  0.0169,  ..., -0.0165,  0.0085,  0.0189],\n",
      "        [-0.0020, -0.0051,  0.0152,  ...,  0.0057, -0.0191, -0.0185],\n",
      "        [ 0.0180, -0.0019, -0.0120,  ..., -0.0041, -0.0158, -0.0215],\n",
      "        ...,\n",
      "        [ 0.0166,  0.0182,  0.0216,  ..., -0.0104, -0.0179, -0.0081],\n",
      "        [-0.0135,  0.0183,  0.0034,  ...,  0.0141,  0.0194,  0.0089],\n",
      "        [-0.0108,  0.0064,  0.0002,  ...,  0.0100,  0.0198,  0.0086]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0121,  0.0141, -0.0337,  ...,  0.0385,  0.0301, -0.0140],\n",
      "        [ 0.0023, -0.0131,  0.0261,  ...,  0.0220,  0.0099, -0.0161],\n",
      "        [ 0.0038, -0.0330, -0.0116,  ...,  0.0045,  0.0382, -0.0126],\n",
      "        ...,\n",
      "        [-0.0313,  0.0436, -0.0388,  ..., -0.0292,  0.0174,  0.0217],\n",
      "        [ 0.0297,  0.0240,  0.0338,  ...,  0.0068, -0.0381, -0.0416],\n",
      "        [-0.0107, -0.0324,  0.0367,  ..., -0.0309, -0.0391,  0.0053]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0184, -0.0086, -0.0157,  ...,  0.0023,  0.0253, -0.0204],\n",
      "        [-0.0068,  0.0441,  0.0032,  ...,  0.0231,  0.0020,  0.0126],\n",
      "        [ 0.0090,  0.0101,  0.0398,  ..., -0.0237, -0.0219,  0.0117],\n",
      "        ...,\n",
      "        [ 0.0393, -0.0186, -0.0240,  ...,  0.0116,  0.0074, -0.0261],\n",
      "        [ 0.0023, -0.0234, -0.0347,  ...,  0.0052,  0.0158,  0.0007],\n",
      "        [ 0.0219, -0.0387, -0.0323,  ..., -0.0374,  0.0027, -0.0003]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0091,  0.0035,  0.0308,  ...,  0.0122,  0.0217,  0.0441],\n",
      "        [-0.0336,  0.0283,  0.0159,  ..., -0.0276, -0.0279, -0.0283],\n",
      "        [-0.0290, -0.0334,  0.0398,  ...,  0.0081,  0.0111, -0.0336],\n",
      "        ...,\n",
      "        [ 0.0090,  0.0148, -0.0181,  ...,  0.0432, -0.0372, -0.0116],\n",
      "        [-0.0075, -0.0372,  0.0384,  ..., -0.0432,  0.0198,  0.0269],\n",
      "        [-0.0091,  0.0245, -0.0440,  ..., -0.0320,  0.0202, -0.0384]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0432,  0.0222,  ...,  0.0241,  0.0324,  0.0065],\n",
      "        [-0.0243,  0.0436,  0.0241,  ...,  0.0068,  0.0233, -0.0327],\n",
      "        [ 0.0366, -0.0072,  0.0288,  ...,  0.0299,  0.0147,  0.0269],\n",
      "        ...,\n",
      "        [ 0.0150,  0.0258, -0.0323,  ...,  0.0004, -0.0241, -0.0119],\n",
      "        [-0.0252, -0.0273,  0.0304,  ...,  0.0370, -0.0247,  0.0341],\n",
      "        [ 0.0143, -0.0218,  0.0024,  ..., -0.0072, -0.0113,  0.0059]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0121,  0.0141, -0.0337,  ...,  0.0385,  0.0301, -0.0140],\n",
      "        [ 0.0023, -0.0131,  0.0261,  ...,  0.0220,  0.0099, -0.0161],\n",
      "        [ 0.0038, -0.0330, -0.0116,  ...,  0.0045,  0.0382, -0.0126],\n",
      "        ...,\n",
      "        [-0.0313,  0.0436, -0.0388,  ..., -0.0292,  0.0174,  0.0217],\n",
      "        [ 0.0297,  0.0240,  0.0338,  ...,  0.0068, -0.0381, -0.0416],\n",
      "        [-0.0107, -0.0324,  0.0367,  ..., -0.0309, -0.0391,  0.0053]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0184, -0.0086, -0.0157,  ...,  0.0023,  0.0253, -0.0204],\n",
      "        [-0.0068,  0.0441,  0.0032,  ...,  0.0231,  0.0020,  0.0126],\n",
      "        [ 0.0090,  0.0101,  0.0398,  ..., -0.0237, -0.0219,  0.0117],\n",
      "        ...,\n",
      "        [ 0.0393, -0.0186, -0.0240,  ...,  0.0116,  0.0074, -0.0261],\n",
      "        [ 0.0023, -0.0234, -0.0347,  ...,  0.0052,  0.0158,  0.0007],\n",
      "        [ 0.0219, -0.0387, -0.0323,  ..., -0.0374,  0.0027, -0.0003]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0091,  0.0035,  0.0308,  ...,  0.0122,  0.0217,  0.0441],\n",
      "        [-0.0336,  0.0283,  0.0159,  ..., -0.0276, -0.0279, -0.0283],\n",
      "        [-0.0290, -0.0334,  0.0398,  ...,  0.0081,  0.0111, -0.0336],\n",
      "        ...,\n",
      "        [ 0.0090,  0.0148, -0.0181,  ...,  0.0432, -0.0372, -0.0116],\n",
      "        [-0.0075, -0.0372,  0.0384,  ..., -0.0432,  0.0198,  0.0269],\n",
      "        [-0.0091,  0.0245, -0.0440,  ..., -0.0320,  0.0202, -0.0384]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0432,  0.0222,  ...,  0.0241,  0.0324,  0.0065],\n",
      "        [-0.0243,  0.0436,  0.0241,  ...,  0.0068,  0.0233, -0.0327],\n",
      "        [ 0.0366, -0.0072,  0.0288,  ...,  0.0299,  0.0147,  0.0269],\n",
      "        ...,\n",
      "        [ 0.0150,  0.0258, -0.0323,  ...,  0.0004, -0.0241, -0.0119],\n",
      "        [-0.0252, -0.0273,  0.0304,  ...,  0.0370, -0.0247,  0.0341],\n",
      "        [ 0.0143, -0.0218,  0.0024,  ..., -0.0072, -0.0113,  0.0059]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0283,  0.0309,  ...,  0.0180,  0.0361,  0.0383],\n",
      "        [-0.0106, -0.0211, -0.0108,  ...,  0.0228, -0.0127, -0.0421],\n",
      "        [ 0.0188,  0.0225,  0.0230,  ..., -0.0391, -0.0187,  0.0121],\n",
      "        ...,\n",
      "        [-0.0186, -0.0052, -0.0300,  ..., -0.0426,  0.0213,  0.0061],\n",
      "        [ 0.0260, -0.0129, -0.0328,  ...,  0.0437, -0.0151, -0.0103],\n",
      "        [-0.0040,  0.0391,  0.0074,  ..., -0.0284,  0.0130, -0.0263]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0197, -0.0189,  0.0169,  ..., -0.0165,  0.0085,  0.0189],\n",
      "        [-0.0020, -0.0051,  0.0152,  ...,  0.0057, -0.0191, -0.0185],\n",
      "        [ 0.0180, -0.0019, -0.0120,  ..., -0.0041, -0.0158, -0.0215],\n",
      "        ...,\n",
      "        [ 0.0166,  0.0182,  0.0216,  ..., -0.0104, -0.0179, -0.0081],\n",
      "        [-0.0135,  0.0183,  0.0034,  ...,  0.0141,  0.0194,  0.0089],\n",
      "        [-0.0108,  0.0064,  0.0002,  ...,  0.0100,  0.0198,  0.0086]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0121,  0.0141, -0.0337,  ...,  0.0385,  0.0301, -0.0140],\n",
      "        [ 0.0023, -0.0131,  0.0261,  ...,  0.0220,  0.0099, -0.0161],\n",
      "        [ 0.0038, -0.0330, -0.0116,  ...,  0.0045,  0.0382, -0.0126],\n",
      "        ...,\n",
      "        [-0.0313,  0.0436, -0.0388,  ..., -0.0292,  0.0174,  0.0217],\n",
      "        [ 0.0297,  0.0240,  0.0338,  ...,  0.0068, -0.0381, -0.0416],\n",
      "        [-0.0107, -0.0324,  0.0367,  ..., -0.0309, -0.0391,  0.0053]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0184, -0.0086, -0.0157,  ...,  0.0023,  0.0253, -0.0204],\n",
      "        [-0.0068,  0.0441,  0.0032,  ...,  0.0231,  0.0020,  0.0126],\n",
      "        [ 0.0090,  0.0101,  0.0398,  ..., -0.0237, -0.0219,  0.0117],\n",
      "        ...,\n",
      "        [ 0.0393, -0.0186, -0.0240,  ...,  0.0116,  0.0074, -0.0261],\n",
      "        [ 0.0023, -0.0234, -0.0347,  ...,  0.0052,  0.0158,  0.0007],\n",
      "        [ 0.0219, -0.0387, -0.0323,  ..., -0.0374,  0.0027, -0.0003]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0091,  0.0035,  0.0308,  ...,  0.0122,  0.0217,  0.0441],\n",
      "        [-0.0336,  0.0283,  0.0159,  ..., -0.0276, -0.0279, -0.0283],\n",
      "        [-0.0290, -0.0334,  0.0398,  ...,  0.0081,  0.0111, -0.0336],\n",
      "        ...,\n",
      "        [ 0.0090,  0.0148, -0.0181,  ...,  0.0432, -0.0372, -0.0116],\n",
      "        [-0.0075, -0.0372,  0.0384,  ..., -0.0432,  0.0198,  0.0269],\n",
      "        [-0.0091,  0.0245, -0.0440,  ..., -0.0320,  0.0202, -0.0384]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0432,  0.0222,  ...,  0.0241,  0.0324,  0.0065],\n",
      "        [-0.0243,  0.0436,  0.0241,  ...,  0.0068,  0.0233, -0.0327],\n",
      "        [ 0.0366, -0.0072,  0.0288,  ...,  0.0299,  0.0147,  0.0269],\n",
      "        ...,\n",
      "        [ 0.0150,  0.0258, -0.0323,  ...,  0.0004, -0.0241, -0.0119],\n",
      "        [-0.0252, -0.0273,  0.0304,  ...,  0.0370, -0.0247,  0.0341],\n",
      "        [ 0.0143, -0.0218,  0.0024,  ..., -0.0072, -0.0113,  0.0059]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0121,  0.0141, -0.0337,  ...,  0.0385,  0.0301, -0.0140],\n",
      "        [ 0.0023, -0.0131,  0.0261,  ...,  0.0220,  0.0099, -0.0161],\n",
      "        [ 0.0038, -0.0330, -0.0116,  ...,  0.0045,  0.0382, -0.0126],\n",
      "        ...,\n",
      "        [-0.0313,  0.0436, -0.0388,  ..., -0.0292,  0.0174,  0.0217],\n",
      "        [ 0.0297,  0.0240,  0.0338,  ...,  0.0068, -0.0381, -0.0416],\n",
      "        [-0.0107, -0.0324,  0.0367,  ..., -0.0309, -0.0391,  0.0053]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0184, -0.0086, -0.0157,  ...,  0.0023,  0.0253, -0.0204],\n",
      "        [-0.0068,  0.0441,  0.0032,  ...,  0.0231,  0.0020,  0.0126],\n",
      "        [ 0.0090,  0.0101,  0.0398,  ..., -0.0237, -0.0219,  0.0117],\n",
      "        ...,\n",
      "        [ 0.0393, -0.0186, -0.0240,  ...,  0.0116,  0.0074, -0.0261],\n",
      "        [ 0.0023, -0.0234, -0.0347,  ...,  0.0052,  0.0158,  0.0007],\n",
      "        [ 0.0219, -0.0387, -0.0323,  ..., -0.0374,  0.0027, -0.0003]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0091,  0.0035,  0.0308,  ...,  0.0122,  0.0217,  0.0441],\n",
      "        [-0.0336,  0.0283,  0.0159,  ..., -0.0276, -0.0279, -0.0283],\n",
      "        [-0.0290, -0.0334,  0.0398,  ...,  0.0081,  0.0111, -0.0336],\n",
      "        ...,\n",
      "        [ 0.0090,  0.0148, -0.0181,  ...,  0.0432, -0.0372, -0.0116],\n",
      "        [-0.0075, -0.0372,  0.0384,  ..., -0.0432,  0.0198,  0.0269],\n",
      "        [-0.0091,  0.0245, -0.0440,  ..., -0.0320,  0.0202, -0.0384]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0432,  0.0222,  ...,  0.0241,  0.0324,  0.0065],\n",
      "        [-0.0243,  0.0436,  0.0241,  ...,  0.0068,  0.0233, -0.0327],\n",
      "        [ 0.0366, -0.0072,  0.0288,  ...,  0.0299,  0.0147,  0.0269],\n",
      "        ...,\n",
      "        [ 0.0150,  0.0258, -0.0323,  ...,  0.0004, -0.0241, -0.0119],\n",
      "        [-0.0252, -0.0273,  0.0304,  ...,  0.0370, -0.0247,  0.0341],\n",
      "        [ 0.0143, -0.0218,  0.0024,  ..., -0.0072, -0.0113,  0.0059]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0283,  0.0309,  ...,  0.0180,  0.0361,  0.0383],\n",
      "        [-0.0106, -0.0211, -0.0108,  ...,  0.0228, -0.0127, -0.0421],\n",
      "        [ 0.0188,  0.0225,  0.0230,  ..., -0.0391, -0.0187,  0.0121],\n",
      "        ...,\n",
      "        [-0.0186, -0.0052, -0.0300,  ..., -0.0426,  0.0213,  0.0061],\n",
      "        [ 0.0260, -0.0129, -0.0328,  ...,  0.0437, -0.0151, -0.0103],\n",
      "        [-0.0040,  0.0391,  0.0074,  ..., -0.0284,  0.0130, -0.0263]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0197, -0.0189,  0.0169,  ..., -0.0165,  0.0085,  0.0189],\n",
      "        [-0.0020, -0.0051,  0.0152,  ...,  0.0057, -0.0191, -0.0185],\n",
      "        [ 0.0180, -0.0019, -0.0120,  ..., -0.0041, -0.0158, -0.0215],\n",
      "        ...,\n",
      "        [ 0.0166,  0.0182,  0.0216,  ..., -0.0104, -0.0179, -0.0081],\n",
      "        [-0.0135,  0.0183,  0.0034,  ...,  0.0141,  0.0194,  0.0089],\n",
      "        [-0.0108,  0.0064,  0.0002,  ...,  0.0100,  0.0198,  0.0086]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2702,  0.9665,  0.8689,  ...,  0.7461, -1.0133, -1.5127],\n",
      "        [ 1.6205, -0.3668,  0.3662,  ...,  1.4504,  0.2733,  0.2208],\n",
      "        [-0.1541,  0.5831,  0.2178,  ..., -0.5265, -2.2544,  0.5922],\n",
      "        ...,\n",
      "        [ 1.0133, -0.1850, -0.9212,  ..., -0.0677, -0.0754,  0.2580],\n",
      "        [-1.3319, -0.5734, -0.7726,  ...,  1.0976,  1.7588, -0.5740],\n",
      "        [ 0.5696,  0.2808, -0.0281,  ...,  0.7650, -0.3095, -0.6587]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2702,  0.9665,  0.8689,  ...,  0.7461, -1.0133, -1.5127],\n",
      "        [ 1.6205, -0.3668,  0.3662,  ...,  1.4504,  0.2733,  0.2208],\n",
      "        [-0.1541,  0.5831,  0.2178,  ..., -0.5265, -2.2544,  0.5922],\n",
      "        ...,\n",
      "        [ 1.0133, -0.1850, -0.9212,  ..., -0.0677, -0.0754,  0.2580],\n",
      "        [-1.3319, -0.5734, -0.7726,  ...,  1.0976,  1.7588, -0.5740],\n",
      "        [ 0.5696,  0.2808, -0.0281,  ...,  0.7650, -0.3095, -0.6587]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0388, -0.0284,  0.0425,  ..., -0.0277,  0.0143, -0.0371],\n",
      "        [ 0.0418, -0.0069, -0.0051,  ...,  0.0425, -0.0338,  0.0015],\n",
      "        [ 0.0197,  0.0210, -0.0086,  ...,  0.0149, -0.0145, -0.0160],\n",
      "        ...,\n",
      "        [-0.0167,  0.0142,  0.0341,  ...,  0.0314, -0.0265,  0.0419],\n",
      "        [ 0.0169, -0.0423, -0.0289,  ..., -0.0025,  0.0426, -0.0061],\n",
      "        [ 0.0408,  0.0442,  0.0043,  ..., -0.0198,  0.0045, -0.0161]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = FullModel(\n",
    "    num_encoder=2,\n",
    "    num_decoder=2,\n",
    "    d_model=512,\n",
    "    vocab_size=11,\n",
    "    num_head=8\n",
    ").cuda()\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        print(p)\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5635], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([-0.1844], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([-0.2768], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([-0.0318], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([4], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 5, 1, 2, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_input = torch.LongTensor([[0, 2, 2, 2, 4]])\n",
    "# decoder_input = torch.LongTensor([[5, 0, 1, 2, 3]])\n",
    "attention_mask = torch.ones(1, 1, mock_input.size(-1))\n",
    "greedy_decode(model, mock_input.cuda(), attention_mask.cuda(), 5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test our model (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 11])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_input = torch.LongTensor([[0, 1, 1, 1, 1, 1, 1, 2, 3, 4]])\n",
    "decoder_input = torch.LongTensor([[0, 0, 1, 1, 1, 1, 1, 1, 2, 3]])\n",
    "attention_mask = torch.ones(1, 1, mock_input.size(-1))\n",
    "\n",
    "output = model(mock_input.cuda(), decoder_input.cuda(), attention_mask.cuda(), get_subsequent_mask(mock_input.size(-1)).unsqueeze(dim=0).cuda())\n",
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = F.linear(output[0], model.shared.embedder.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generator.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold = torch.zeros(10, dtype=torch.long)\n",
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8,  8,  8,  9,  8,  9, 10,  9, 10,  8]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "I will directly use tools from pytorch to train the model.\n",
    "Here are the things we need:\n",
    "- a module to manage and split our data -> Dataset and DataLoader\n",
    "- a module to optimize our model based on the loss -> optimizer\n",
    "- a module to manage the learning rate we will use -> scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randint(1, 11, (20000, 10))\n",
    "data[:, 0] = 1\n",
    "src = data.requires_grad_(False).clone().detach()\n",
    "tgt = data.requires_grad_(False).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fct = nn.KLDivLoss(reduction='sum')\n",
    "from utils import LabelSmoothing\n",
    "# loss_fct = LabelSmoothing(size=5, smoothing=0.1)\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(\n",
    "    model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda step: rate(\n",
    "    step, model_size=model.d_model, factor=1, warmup=400\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7257)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 6, 5)\n",
    "test_tgt = torch.randint(0, 5, (2, 6))\n",
    "test_loss = loss_fct(a.view(-1, a.size(-1)), test_tgt.flatten())\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyDataset(Dataset):\n",
    "    def __init__(self, raw_data):\n",
    "        super().__init__()\n",
    "        self.data = raw_data\n",
    "        # self.bos = torch.tensor([0])\n",
    "        # self.eos = torch.tensor([6])\n",
    "        self.pad = torch.tensor([11])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_item = self.data[index]\n",
    "        src = data_item\n",
    "        # tgt = torch.cat([self.bos, data_item[:-1]], dim=-1)\n",
    "        # tgt_y = torch.cat([data_item[1:], self.eos], dim=-1)\n",
    "        tgt = data_item[:-1]\n",
    "        tgt_y = data_item[1:]\n",
    "\n",
    "        encoder_attention_mask = torch.ones(1, 1).type_as(src).masked_fill(src == self.pad, 0)\n",
    "        decoder_pad_mask = torch.ones(1, 1).type_as(tgt).masked_fill(tgt == self.pad, 0)\n",
    "        decoder_subsequent_mask = get_subsequent_mask(tgt.size(-1))\n",
    "        decoder_attention_mask = decoder_pad_mask & decoder_subsequent_mask\n",
    "        return {\n",
    "            'encoder_input_ids': src,\n",
    "            'decoder_input_ids': tgt,\n",
    "            'target_ids': tgt_y,\n",
    "            'encoder_attention_mask': encoder_attention_mask,\n",
    "            'decoder_attention_mask': decoder_attention_mask\n",
    "        }\n",
    "\n",
    "def split_data(data):\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    val_size = len(data) - train_size\n",
    "    train, val = torch.utils.data.random_split(data, [train_size, val_size])\n",
    "\n",
    "    train_dataset = CopyDataset(train)\n",
    "    val_dataset = CopyDataset(val)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_input_ids': tensor([1, 2, 6, 5, 1, 6, 5, 9, 4, 2]),\n",
       " 'decoder_input_ids': tensor([1, 2, 6, 5, 1, 6, 5, 9, 4, 2]),\n",
       " 'target_ids': tensor([1, 2, 6, 5, 1, 6, 5, 9, 4, 2]),\n",
       " 'encoder_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'decoder_attention_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=80, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:00<00:00, 33.56it/s, loss=tensor(2.9739, device='cuda:0', grad_fn=<NllLossBackward0>), lr=1.93e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:06, 28.73it/s, val_loss=tensor(4.3297, device='cuda:0')]                                                           \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:07, 28.14it/s, val_loss=tensor(4.3977, device='cuda:0')]-05, device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000572]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:00, 36.74it/s, loss=tensor(4.0692e-05, device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.0011]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:06, 28.69it/s, val_loss=tensor(4.6182, device='cuda:0')]                                                               \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:12, 15.71it/s, val_loss=tensor(5.2185, device='cuda:0')]-07, device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000898]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  8.89it/s, loss=tensor(2.1458e-08, device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000781]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:19, 10.47it/s, val_loss=tensor(5.2253, device='cuda:0')]                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:08, 23.56it/s, val_loss=tensor(5.3536, device='cuda:0')]\n",
      " 35%|███▌      | 7/20 [00:00<00:00, 37.98it/s, loss=tensor(1.4901e-10, device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000636]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:14, 13.58it/s, val_loss=tensor(5.4419, device='cuda:0')]                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:19, 10.20it/s, val_loss=tensor(5.4324, device='cuda:0')]\n",
      " 10%|█         | 2/20 [00:00<00:01, 12.47it/s, loss=tensor(0., device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:19, 10.32it/s, val_loss=tensor(5.4596, device='cuda:0')]                                                                \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:19, 10.50it/s, val_loss=tensor(5.4570, device='cuda:0')]ice='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000521]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 11.64it/s, loss=tensor(0., device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000494]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:20,  9.60it/s, val_loss=tensor(5.4795, device='cuda:0')]                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:20,  9.54it/s, val_loss=tensor(5.5764, device='cuda:0')]\n",
      " 10%|█         | 2/20 [00:00<00:01, 12.36it/s, loss=tensor(0., device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:17, 11.30it/s, val_loss=tensor(5.6016, device='cuda:0')]                                                        \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:07, 28.51it/s, val_loss=tensor(5.5962, device='cuda:0')]ice='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000433]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:00, 40.19it/s, loss=tensor(0., device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000417]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:06, 29.36it/s, val_loss=tensor(5.5943, device='cuda:0')]                                                               \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:06, 29.35it/s, val_loss=tensor(5.5868, device='cuda:0')]ice='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000403]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 38.16it/s, loss=tensor(0., device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.00039] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:06, 30.05it/s, val_loss=tensor(5.5923, device='cuda:0')]                                                        \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:06, 29.81it/s, val_loss=tensor(5.5911, device='cuda:0')]ice='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000379]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:00, 36.35it/s, loss=tensor(0., device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:06, 30.63it/s, val_loss=tensor(5.5891, device='cuda:0')]                                                        \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:00<00:00, 36.26it/s, loss=tensor(0., device='cuda:0', grad_fn=<NllLossBackward0>), lr=0.000358]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    pbar = tqdm(total=20)\n",
    "    print(\"Epoch #{}\".format(epoch))\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        encoder_input_ids = batch['encoder_input_ids'].cuda()\n",
    "        decoder_input_ids = batch['decoder_input_ids'].cuda()\n",
    "        target_ids = batch['target_ids'].cuda()\n",
    "        encoder_attention_mask = batch['encoder_attention_mask'].cuda()\n",
    "        decoder_attention_mask = batch['decoder_attention_mask'].cuda()\n",
    "        logits, pred = model(encoder_input_ids, decoder_input_ids, encoder_attention_mask, decoder_attention_mask)\n",
    "\n",
    "        output = model.generate(logits)\n",
    "\n",
    "        loss = loss_fct(output.view(-1, output.size(-1)), target_ids.view(target_ids.flatten().size(0)))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({'loss': loss, 'lr': optimizer.param_groups[0][\"lr\"]}, refresh=True)\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for batch in val_loader:\n",
    "        encoder_input_ids = batch['encoder_input_ids'].cuda()\n",
    "        decoder_input_ids = batch['decoder_input_ids'].cuda()\n",
    "        target_ids = batch['target_ids'].cuda()\n",
    "        encoder_attention_mask = batch['encoder_attention_mask'].cuda()\n",
    "        decoder_attention_mask = batch['decoder_attention_mask'].cuda()\n",
    "        logits, pred = model(encoder_input_ids, decoder_input_ids, encoder_attention_mask, decoder_attention_mask)\n",
    "    \n",
    "        loss += loss_fct(pred.view(-1, pred.size(-1)), target_ids.view(target_ids.flatten().size(0))).detach()\n",
    "    pbar.set_postfix({'val_loss': loss / len(val_loader)}, refresh=True)\n",
    "        # print(optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0022097086912079614\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 9, 5, 5, 6, 9, 1, 4, 7, 3]),\n",
       " tensor([1, 9, 5, 5, 6, 9, 1, 4, 7, 3]),\n",
       " tensor([1, 9, 5, 5, 6, 9, 1, 4, 7, 3]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['target_ids'][0], batch['decoder_input_ids'][0], batch['encoder_input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 9, 5, 5, 6, 9, 1, 4, 7, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['decoder_input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 11])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.1544, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(3.1544, device='cuda:0', grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = 0\n",
    "for i in range(pred.size(0)):\n",
    "    loss += loss_fct(pred[i], target_ids[i])\n",
    "loss = loss / pred.size(0)\n",
    "loss, loss_fct(pred.view(-1, pred.size(-1)), target_ids.view(target_ids.flatten().size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1544, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct(pred.view(-1, pred.size(-1)), target_ids.view(target_ids.flatten().size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids.view(target_ids.flatten().size(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3400\\391859648.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: zeros() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "torch.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  7,  7,  7,  8,  1,  2, 10,  4,  9,  1,  5,  6,  8,  6,  4,  4,  6,\n",
       "         7,  8], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids.view(target_ids.flatten().size(0))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.1544, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(3.1544, device='cuda:0', grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, loss_fct(torch.cat([item for item in pred]), torch.cat([item for item in target_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7735,  1.6937,  1.8926, -1.2290, -2.1162,  0.6710, -0.8189, -1.3710,\n",
       "          1.7014,  0.1233, -0.0146],\n",
       "        [-2.7066,  0.5713,  0.7198, -0.1147, -0.8357, -1.7156, -0.9016,  0.3126,\n",
       "         -0.6881,  1.7899,  2.4643]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.view(-1, pred.size(-1))[10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_input = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "# decoder_input = torch.LongTensor([[5, 0, 1, 2, 3]])\n",
    "attention_mask = torch.ones(1, 1, mock_input.size(-1))\n",
    "# decoder_attention_mask = get_subsequent_mask(mock_input.size(-1)).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullModel(\n",
       "  (shared): Embedding(\n",
       "    (embedder): Embedding(11, 512)\n",
       "  )\n",
       "  (model): EncoderDecoder(\n",
       "    (encoder): Encoder(\n",
       "      (layer_list): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (attn): MultiHeadAttention(\n",
       "            (layernorm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ffn): FeedForwardNetwork(\n",
       "            (layernorm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ffn): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Dropout(p=0.5, inplace=False)\n",
       "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): EncoderLayer(\n",
       "          (attn): MultiHeadAttention(\n",
       "            (layernorm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ffn): FeedForwardNetwork(\n",
       "            (layernorm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ffn): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Dropout(p=0.5, inplace=False)\n",
       "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (layer_list): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (attn): MultiHeadAttention(\n",
       "            (layernorm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attn): MultiHeadAttention(\n",
       "            (layernorm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ffn): FeedForwardNetwork(\n",
       "            (layernorm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ffn): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Dropout(p=0.5, inplace=False)\n",
       "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (attn): MultiHeadAttention(\n",
       "            (layernorm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (cross_attn): MultiHeadAttention(\n",
       "            (layernorm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ffn): FeedForwardNetwork(\n",
       "            (layernorm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ffn): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Dropout(p=0.5, inplace=False)\n",
       "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (embedder): Sequential(\n",
       "      (0): Embedding(\n",
       "        (embedder): Embedding(11, 512)\n",
       "      )\n",
       "      (1): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (tgt_embedder): Sequential(\n",
       "      (0): Embedding(\n",
       "        (embedder): Embedding(11, 512)\n",
       "      )\n",
       "      (1): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (generator): Generator(\n",
       "      (proj): Linear(in_features=512, out_features=11, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]),\n",
       " tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask, mock_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3018], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([0.], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([0.], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([0.], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([0.], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([0.], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([0.], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([0.], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([0.], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "tensor([9], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 9, 9, 9, 9, 9, 9, 9, 9, 9]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_decode(model, mock_input.cuda(), attention_mask.cuda(), 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0083, -0.0345,  0.0599,  ..., -0.0130,  0.0741, -0.0066],\n",
       "        [ 0.0338,  0.0370, -0.0141,  ...,  0.0455,  0.0997,  0.0742],\n",
       "        [ 0.0604, -0.0374,  0.0189,  ..., -0.0454,  0.0603, -0.0203],\n",
       "        ...,\n",
       "        [ 0.0464, -0.0650,  0.0247,  ...,  0.0020,  0.0803,  0.0235],\n",
       "        [ 0.0016,  0.0587,  0.0922,  ...,  0.0712, -0.0785, -0.0860],\n",
       "        [-0.0795,  0.0527, -0.0127,  ...,  0.0185,  0.0922,  0.1018]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model.shared.embedder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
