{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "There have been plenty of well-organized tutorials elaborating on details of the Transformer. This one is inpired by and based on annotated-transformer from the Harvard NLP group, which is a great tutorial showing everything you need to reproduce the transformer model from paper. However, from a beginner's standpoint, it is sometimes easy to get lost when stuck with an unfamiliar concept and need to go for further readings. In this notebook, I try to alleviate this by organizing the codes in a top-down manner. And instead of using texts from the original paper of transfomer, I will explain using my own words and provide links to useful resources for each module if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from feedforward import FeedForwardNetwork\n",
    "from multiheadattention import MultiHeadAttention\n",
    "from utils import clone, PositionalEncoding, Embedding, get_subsequent_mask, rate\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple task\n",
    "Firstly, we want to know what our task is. We take the same task as in annotated-transformer, which is to memorize the sequence of numbers from 1 to 10. Therefore, the size of our vocabulary should be 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_encoder=6, \n",
    "            num_decoder=6, \n",
    "            d_model=512, \n",
    "            vocab_size=13,\n",
    "            num_head=6,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        c = deepcopy\n",
    "        ffn = FeedForwardNetwork(d_model)\n",
    "        attn = MultiHeadAttention(d_model=d_model, num_head=num_head)\n",
    "        self.d_model = d_model\n",
    "        self.shared = Embedding(vocab=vocab_size, d_model=d_model)\n",
    "        self.model = EncoderDecoder(\n",
    "            Encoder(EncoderLayer(c(attn), c(ffn)), num_layers=num_encoder),\n",
    "            Decoder(DecoderLayer(c(attn), c(attn), c(ffn)), num_layers=num_decoder),\n",
    "            nn.Sequential(self.shared,\n",
    "                          PositionalEncoding(d_model=d_model)),\n",
    "        )\n",
    "\n",
    "    def forward(self, src_input, tgt_input, src_mask, tgt_mask):\n",
    "        logits = self.model(src_input, tgt_input, src_mask, tgt_mask)\n",
    "        sequence = F.linear(logits, self.shared.embedder.weight)\n",
    "        return (logits, sequence)\n",
    "\n",
    "    def generate(self, src_embed, src_mask=None, tgt_embed=None, tgt_mask=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def forward(self, src_input, tgt_input, src_mask, tgt_mask):\n",
    "        memory = self.encode(self.embedder(src_input), src_mask)\n",
    "        return self.decode(memory, src_mask, self.embedder(tgt_input), tgt_mask)\n",
    "\n",
    "    def encode(self, src_embed, src_mask):\n",
    "        return self.encoder(src_embed, src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt_embed, tgt_mask):\n",
    "        return self.decoder(memory, src_mask, tgt_embed, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layer_list = clone(layer, num_layers)\n",
    "\n",
    "    def forward(self, src_embed, src_mask):\n",
    "        x = src_embed\n",
    "        for layer in self.layer_list:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attn, ffn):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.ffn = ffn\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.attn(x, x, x, mask)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layer_list = clone(layer, num_layers)\n",
    "\n",
    "    def forward(self, memory, src_mask, tgt_embed, tgt_mask):\n",
    "        x = tgt_embed\n",
    "        for layer in self.layer_list:\n",
    "            x = layer(memory, src_mask, tgt_embed, tgt_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, attn, cross_attn, ffn):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.cross_attn = cross_attn\n",
    "        self.ffn = ffn\n",
    "\n",
    "    def forward(self, m, src_mask, x, tgt_mask):\n",
    "        x = self.attn(x, x, x, tgt_mask)\n",
    "        x = self.cross_attn(x, m, m, src_mask)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullModel(\n",
    "    num_encoder=3,\n",
    "    num_decoder=3,\n",
    "    d_model=64,\n",
    "    vocab_size=13,\n",
    "    num_head=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test our model (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 13])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_input = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "attention_mask = torch.ones(1, 1, mock_input.size(-1))\n",
    "\n",
    "output = model(mock_input, mock_input, attention_mask, get_subsequent_mask(mock_input.size(-1)).unsqueeze(dim=0))\n",
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = F.linear(output[0], model.shared.embedder.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generator.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold = torch.zeros(10, dtype=torch.long)\n",
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.CrossEntropyLoss()\n",
    "loss = loss_fct(output[0][0][0:10], gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "I will directly use tools from pytorch to train the model.\n",
    "Here are the things we need:\n",
    "- a module to manage and split our data -> Dataset and DataLoader\n",
    "- a module to optimize our model based on the loss -> optimizer\n",
    "- a module to manage the learning rate we will use -> scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randint(0, 10, (100000, 10))\n",
    "src = data.requires_grad_(False).clone().detach()\n",
    "tgt = data.requires_grad_(False).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(\n",
    "    model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda step: rate(\n",
    "    step, model_size=model.d_model, factor=1.0, warmup=400\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyDataset(Dataset):\n",
    "    def __init__(self, raw_data):\n",
    "        super().__init__()\n",
    "        self.data = raw_data\n",
    "        self.bos = torch.tensor([10])\n",
    "        self.eos = torch.tensor([11])\n",
    "        self.pad = torch.tensor([12])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_item = self.data[index]\n",
    "        src = data_item\n",
    "        tgt = torch.cat([self.bos, data_item[:-1]], dim=-1)\n",
    "        tgt_y = torch.cat([data_item[1:], self.eos], dim=-1)\n",
    "\n",
    "        encoder_attention_mask = torch.ones(1, 1).type_as(src).masked_fill(src == self.pad, 0)\n",
    "        decoder_pad_mask = torch.ones(1, 1).type_as(tgt).masked_fill(tgt == self.pad, 0)\n",
    "        decoder_subsequent_mask = get_subsequent_mask(tgt.size(-1))\n",
    "        decoder_attention_mask = decoder_pad_mask & decoder_subsequent_mask\n",
    "        return {\n",
    "            'encoder_input_ids': src,\n",
    "            'decoder_input_ids': tgt,\n",
    "            'target_ids': tgt_y,\n",
    "            'encoder_attention_mask': encoder_attention_mask,\n",
    "            'decoder_attention_mask': decoder_attention_mask\n",
    "        }\n",
    "\n",
    "def split_data(data):\n",
    "    train_size = int(len(data) * 0.7)\n",
    "    val_size = len(data) - train_size\n",
    "    train, val = torch.utils.data.random_split(data, [train_size, val_size])\n",
    "\n",
    "    train_dataset = CopyDataset(train)\n",
    "    val_dataset = CopyDataset(val)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for batch in train_loader:\n",
    "        encoder_input_ids = batch['encoder_input_ids']\n",
    "        decoder_input_ids = batch['decoder_input_ids']\n",
    "        target_ids = batch['target_ids']\n",
    "        encoder_attention_mask = batch['encoder_attention_mask']\n",
    "        decoder_attention_mask = batch['decoder_attention_mask']\n",
    "        logits, pred = model(encoder_input_ids, decoder_input_ids, encoder_attention_mask, decoder_attention_mask)\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 10, 64]), torch.Size([20, 10]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, target_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10, 13])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  5,  4,  9,  3,  9,  2,  2,  8, 11])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 13])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.2004, grad_fn=<AddBackward0>)\n",
      "tensor(30.4008, grad_fn=<AddBackward0>)\n",
      "tensor(45.6012, grad_fn=<AddBackward0>)\n",
      "tensor(60.8017, grad_fn=<AddBackward0>)\n",
      "tensor(76.0021, grad_fn=<AddBackward0>)\n",
      "tensor(91.2025, grad_fn=<AddBackward0>)\n",
      "tensor(106.4029, grad_fn=<AddBackward0>)\n",
      "tensor(121.6033, grad_fn=<AddBackward0>)\n",
      "tensor(136.8037, grad_fn=<AddBackward0>)\n",
      "tensor(152.0042, grad_fn=<AddBackward0>)\n",
      "tensor(167.2046, grad_fn=<AddBackward0>)\n",
      "tensor(182.4050, grad_fn=<AddBackward0>)\n",
      "tensor(197.6054, grad_fn=<AddBackward0>)\n",
      "tensor(212.8058, grad_fn=<AddBackward0>)\n",
      "tensor(228.0062, grad_fn=<AddBackward0>)\n",
      "tensor(243.2066, grad_fn=<AddBackward0>)\n",
      "tensor(258.4070, grad_fn=<AddBackward0>)\n",
      "tensor(273.6074, grad_fn=<AddBackward0>)\n",
      "tensor(288.8078, grad_fn=<AddBackward0>)\n",
      "tensor(304.0082, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(15.2004, grad_fn=<DivBackward0>),\n",
       " tensor(15.0956, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = 0\n",
    "for i in range(pred.size(0)):\n",
    "    loss += loss_fct(pred[0], target_ids[0])\n",
    "    print(loss)\n",
    "loss = loss / pred.size(0)\n",
    "loss, loss_fct(pred.view(-1, pred.size(-1))[10:20], target_ids.view(target_ids.flatten().size(0))[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  4,  6,  8,  6,  5,  6,  8,  4, 11])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  5,  4,  9,  3,  9,  2,  2,  8, 11,  1,  4,  6,  8,  6,  5,  6,  8,\n",
       "         4, 11])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids.view(target_ids.flatten().size(0))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2.1049,  -9.9884, -19.3442, -14.0272,   0.3019,  -7.3749,  -9.4933,\n",
       "           6.7276,   3.3995,  11.9049,   0.5803,   0.9559,  -1.6682],\n",
       "        [ -3.9587,  -7.3034, -24.4023,  -7.5417,  -3.5900,  -4.8015, -18.0499,\n",
       "           2.1524,   1.8335,  16.4749,   6.6630,   7.4913,   5.5067],\n",
       "        [ -1.5124,  -4.5905, -23.4429,  -8.2280,   2.8414,  -4.8726,  -6.4692,\n",
       "           4.1162,  -0.2039,  10.4359,  -2.1077,  -7.4631,   0.4272],\n",
       "        [ -8.5447,  -7.5131, -28.0028,  -9.9923,   0.6588, -10.6035, -12.4545,\n",
       "           7.0173,   4.8939,   9.4018,   0.8496,   2.9447,   2.5613],\n",
       "        [ -2.6194,  -6.3957, -26.9619,  -8.0889,  -4.8687,  -7.4488, -11.8708,\n",
       "           1.8224,   4.4257,   8.7080,   2.5808,   2.2764,   1.2461],\n",
       "        [ -2.0358,  -4.9837, -14.8687,  -4.9099,   0.7512,  -4.7846,  -3.9062,\n",
       "           0.9703,   4.8111,  15.9786,  -4.5040,  -2.3686,   4.9350],\n",
       "        [ -5.9467, -13.8148, -21.9168, -11.7043,  -5.3466, -10.3417, -11.6005,\n",
       "           2.1883,   2.8277,   9.9391,   0.1109,   2.3191,   3.1573],\n",
       "        [ -2.6931, -11.6006, -17.4290, -10.4210,   0.4514,  -4.6751, -13.0076,\n",
       "           7.3396,   3.7496,   6.0033,  -0.6269,  -0.9714,  -1.0516],\n",
       "        [ -2.6343,  -4.3066, -16.3490,  -7.5113,  -2.1352,   1.4488, -13.8522,\n",
       "           8.2632,   1.0854,   7.6253,  -2.1929,  -1.7353,   5.1002],\n",
       "        [ -3.4664,  -6.7324, -18.2790, -11.1020,  -1.9854,  -0.3271, -12.9603,\n",
       "           5.9649,   7.4926,   9.4477,  -1.6434,  -0.2575,   5.0213]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2.1049,  -9.9884, -19.3442, -14.0272,   0.3019,  -7.3749,  -9.4933,\n",
       "           6.7276,   3.3995,  11.9049,   0.5803,   0.9559,  -1.6682],\n",
       "        [ -3.9587,  -7.3034, -24.4023,  -7.5417,  -3.5900,  -4.8015, -18.0499,\n",
       "           2.1524,   1.8335,  16.4749,   6.6630,   7.4913,   5.5067]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.view(-1, pred.size(-1))[10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 13])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.view(-1, pred.size(-1)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
