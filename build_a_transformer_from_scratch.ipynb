{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "There have been plenty of well-organized tutorials elaborating on details of the Transformer. This one is inpired by and based on annotated-transformer from the Harvard NLP group, which is a great tutorial showing everything you need to reproduce the transformer model from paper. However, from a beginner's standpoint, it is sometimes easy to get lost when stuck with an unfamiliar concept and need to go for further readings. In this notebook, I try to alleviate this by organizing the codes in a top-down manner. And instead of using texts from the original paper of transfomer, I will explain using my own words and provide links to useful resources for each module if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from feedforward import FeedForwardNetwork\n",
    "from multiheadattention import MultiHeadAttention\n",
    "from utils import clone, PositionalEncoding, Embedding, get_subsequent_mask\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple task\n",
    "Firstly, we want to know what our task is. We take the same task as in annotated-transformer, which is to memorize the sequence of numbers from 1 to 10. Therefore, the size of our vocabulary should be 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_encoder=6, \n",
    "            num_decoder=6, \n",
    "            d_model=512, \n",
    "            vocab_size=10,\n",
    "            num_head=6,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        c = deepcopy\n",
    "        ffn = FeedForwardNetwork(d_model)\n",
    "        attn = MultiHeadAttention(d_model=d_model, num_head=num_head)\n",
    "        self.shared = Embedding(vocab=vocab_size, d_model=d_model)\n",
    "        self.model = EncoderDecoder(\n",
    "            Encoder(EncoderLayer(c(attn), c(ffn)), num_layers=num_encoder),\n",
    "            Decoder(DecoderLayer(c(attn), c(attn), c(ffn)), num_layers=num_decoder),\n",
    "            nn.Sequential(self.shared,\n",
    "                          PositionalEncoding(d_model=d_model)),\n",
    "        )\n",
    "\n",
    "    def forward(self, src_input, tgt_input, src_mask, tgt_mask):\n",
    "        return self.model(src_input, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "    def generate(self, src_embed, src_mask=None, tgt_embed=None, tgt_mask=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def forward(self, src_input, tgt_input, src_mask, tgt_mask):\n",
    "        memory = self.encode(self.embedder(src_input), src_mask)\n",
    "        return self.decode(memory, src_mask, self.embedder(tgt_input), tgt_mask)\n",
    "\n",
    "    def encode(self, src_embed, src_mask):\n",
    "        return self.encoder(src_embed, src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt_embed, tgt_mask):\n",
    "        return self.decoder(memory, src_mask, tgt_embed, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layer_list = clone(layer, num_layers)\n",
    "\n",
    "    def forward(self, src_embed, src_mask):\n",
    "        x = src_embed\n",
    "        for layer in self.layer_list:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attn, ffn):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.ffn = ffn\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.attn(x, x, x, mask)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layer_list = clone(layer, num_layers)\n",
    "\n",
    "    def forward(self, memory, src_mask, tgt_embed, tgt_mask):\n",
    "        x = tgt_embed\n",
    "        for layer in self.layer_list:\n",
    "            x = layer(memory, src_mask, tgt_embed, tgt_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, attn, cross_attn, ffn):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.cross_attn = cross_attn\n",
    "        self.ffn = ffn\n",
    "\n",
    "    def forward(self, m, src_mask, x, tgt_mask):\n",
    "        x = self.attn(x, x, x, tgt_mask)\n",
    "        x = self.cross_attn(x, m, m, src_mask)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullModel(\n",
    "    num_encoder=3,\n",
    "    num_decoder=3,\n",
    "    d_model=64,\n",
    "    vocab_size=10,\n",
    "    num_head=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test our model (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_input = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "attention_mask = torch.ones(1, 1, mock_input.size(-1))\n",
    "\n",
    "output = model(mock_input, mock_input, attention_mask, get_subsequent_mask(mock_input.size(-1)).unsqueeze(dim=0))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = F.linear(output, model.shared.embedder.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generator.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold = torch.zeros(10, dtype=torch.long)\n",
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.CrossEntropyLoss()\n",
    "loss = loss_fct(output[0][0:10], gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "I will directly use tools from pytorch to train the model.\n",
    "Here are the things we need:\n",
    "- a module to manage and split our data -> Dataset and DataLoader\n",
    "- a module to optimize our model based on the loss -> optimizer\n",
    "- a module to manage the learning rate we will use -> scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import LinearLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randint(0, 10, (100000,))\n",
    "src = data.requires_grad_(False).clone().detach()\n",
    "tgt = data.requires_grad_(False).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
